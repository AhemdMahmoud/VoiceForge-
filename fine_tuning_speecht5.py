# -*- coding: utf-8 -*-
"""Fine-tuning SpeechT5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jGxgq-N-k6oQwkKYq1UrbP5a9mtDZwAK
"""

! nvidia-smi

! pip install transformers datasets soundfile speechbrain accelerate

from datasets import load_dataset, Audio
from transformers import SpeechT5ForTextToSpeech, SpeechT5Processor

from huggingface_hub import notebook_login

notebook_login()

datasets = load_dataset("facebook/voxpopuli", "nl", split="train")

datasets

datasets[0]["audio"]

datasets.cast_column("audio",Audio(sampling_rate=16000))

model = SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts")
processor = SpeechT5Processor.from_pretrained("microsoft/speecht5_tts")

model.config

processor

tokenizer = processor.tokenizer

datasets[0]

datasets[0]["normalized_text"]

"""Because SpeechT5 was trained on the English language, it may not recognize certain characters in the Dutch dataset. If left as is, these characters will be converted to <unk> tokens. However, in Dutch, certain characters like à are used to stress syllables. In order to preserve the meaning of the text, we can replace this character with a regular a."""

def extract_all_chars(batch):
  all_text = "".join(batch["normalized_text"])
  vocab = list(set(all_text))
  return {"vocab":[vocab], "all_text":[all_text]}

vocabs = datasets.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=datasets.column_names)

vocabs

vocabs[0]["vocab"]

dataset_vocab = set(vocabs["vocab"][0])

dataset_vocab

dataset_vocab

tokenizer_vocab = {k for k,_ in tokenizer.get_vocab().items()}

tokenizer_vocab

"""### `Observe and discover` difference between token in model` e and  é `in data which we want to finetuing on it"""

dataset_vocab - tokenizer_vocab

replacements = [
    ("à", "a"),
    ("ç", "c"),
    ("è", "e"),
    ("ë", "e"),
    ("í", "i"),
    ("ï", "i"),
    ("ö", "o"),
    ("ü", "u"),
]

test= datasets[0]["normalized_text"]

test

def clean_up (inputs):
  for src, dest in replacements:
    inputs["normalized_text"] = inputs["normalized_text"].replace(src, dest)
  return inputs

datasets = datasets.map(clean_up)

datasets["normalized_text"][2]

"""# Speakers"""

datasets

datasets[2]["speaker_id"]

from collections import defaultdict

speaker_counts = defaultdict(int)


for speaker_id in datasets["speaker_id"]:
  speaker_counts[speaker_id] +=1

speaker_counts

#visulization

import matplotlib.pyplot as plt

plt.figure()

plt.hist(speaker_counts.values(),bins=20)
plt.ylabel("Speakers")
plt.xlabel("Examples")
plt.show()

def select_speaker(speaker_id):
  return 100 <= speaker_counts[speaker_id] <= 400
datasets = datasets.filter(select_speaker,input_columns=["speaker_id"])

len(set(datasets["speaker_id"]))

len(datasets)

"""### You are left with just under 10,000 examples from approximately 40 unique speakers, which should be sufficient.

## Speaker embeddings
"""

import os
import torch
from speechbrain.pretrained import EncoderClassifier


spk_model_name = "speechbrain/spkrec-xvect-voxceleb"

device = "cuda" if torch.cuda.is_available() else "cpu"

spk_model = EncoderClassifier.from_hparams(
    spk_model_name,
    run_opts = {"device":device},
    savedir = os.path.join("/temp",spk_model_name)
)

def create_speaker_embedding(wavform):
  with torch.no_grad():
    speaker_embedding = spk_model.encode_batch(torch.tensor(wavform))
    speaker_embedding = torch.nn.functional.normalize(speaker_embedding,dim=2).squeeze().cpu().numpy()

  return speaker_embedding

datasets

def prepare_dataset(example):
  audio = example["audio"]

  example = processor(
        text=example["normalized_text"],
        audio_target=audio["array"],
        sampling_rate=audio["sampling_rate"],
        return_attention_mask=False,
  )
  example["labels"] = example["labels"][0]

  example["speaker_embeddings"] = create_speaker_embedding(audio["array"])

  return example

processed_example = prepare_dataset(datasets[0])

processed_example

list(processed_example.keys())

processed_example["speaker_embeddings"].shape

plt.figure()
plt.imshow(processed_example["labels"].T,origin='lower')
plt.show()

datasets = datasets.map(prepare_dataset,remove_columns=datasets.column_names)

for i in range(450,460):
  s=len(datasets[i]["input_ids"])
  print(s)

processor.tokenizer.model_max_length

"""### You’ll see a warning saying that some examples in the dataset are longer than the maximum input length the model can handle (600 tokens). Remove those examples from the dataset. Here we go even further and to allow for larger batch sizes we remove anything over 200 tokens."""

def is_not_too_long(input_ids):
    input_length = len(input_ids)
    return input_length < 200  # Changed from processor.tokenizer.model_max_length to 200


dataset = datasets.filter(is_not_too_long, input_columns=["input_ids"])
len(dataset)

datasets

datasets = datasets.train_test_split(test_size=0.1)

datasets

"""# Data collator"""

from dataclasses import dataclass
from typing import List, Dict, Union, Any

@dataclass
class TTSDataCollatorWithPadding:
    processor: Any

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        input_ids = [{"input_ids": feature["input_ids"]} for feature in features]
        label_features = [{"input_values": feature["labels"]} for feature in features]
        speaker_features = [feature["speaker_embeddings"] for feature in features]

        # collate the inputs and targets into a batch
        # Add padding=True to ensure consistent input lengths
        batch = processor.pad(
            input_ids=input_ids, labels=label_features, return_tensors="pt", padding=True
        )

        # replace padding with -100 to ignore loss correctly
        batch["labels"] = batch["labels"].masked_fill(
            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100
        )

        # not used during fine-tuning
        del batch["decoder_attention_mask"]

        # round down target lengths to multiple of reduction factor
        if model.config.reduction_factor > 1:
            target_lengths = torch.tensor(
                [len(feature["input_values"]) for feature in label_features]
            )
            target_lengths = target_lengths.new(
                [
                    length - length % model.config.reduction_factor
                    for length in target_lengths
                ]
            )
            max_length = max(target_lengths)
            batch["labels"] = batch["labels"][:, :max_length]

        # also add in the speaker embeddings
        batch["speaker_embeddings"] = torch.tensor(speaker_features)

        return batch

data_collator = TTSDataCollatorWithPadding(processor=processor)

"""### Train the model"""

from functools import partial

model.config.use_cache = False

model.generate = partial(model.generate, use_cache = False)

datasets

datasets["train"][0]["labels"]

from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer

args = Seq2SeqTrainingArguments(
        output_dir="speecht5_finetuned_voxpopuli_nl",  # change to a repo name of your choice
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,
        learning_rate=1e-5,
        warmup_steps=500,
        max_steps=4000,
        gradient_checkpointing=True,
        fp16=True,
        # evaluation_strategy="steps",
        per_device_eval_batch_size=2,
        save_steps=1000,
        eval_steps=1000,
        logging_steps=25,
        report_to=["tensorboard"],
        # load_best_model_at_end=True,
        greater_is_better=False,
        label_names=["labels"],
        push_to_hub=True,
    )

trainer = Seq2SeqTrainer(
    args=args,
    model=model,
    train_dataset=datasets["train"],
    eval_dataset=datasets["test"],
    data_collator=data_collator,
    tokenizer=processor,
)

trainer.train()

datasets

processor(
        text=datasets[0]["normalized_text"],
        audio_target=datasets[0]["audio"]["array"],
        sampling_rate=datasets[0]["audio"]["sampling_rate"],
        return_attention_mask=False,
  )

"""# Inference"""

model = SpeechT5ForTextToSpeech.from_pretrained(
    "A7med/speecht5_finetuned_voxpopuli_nl"
)

example = datasets["test"][34]
speaker_embeddings = torch.tensor(example["speaker_embeddings"]).unsqueeze(0)

text = "hallo allemaal, ik praat nederlands. groetjes aan iedereen!"

inputs = processor(text=text, return_tensors="pt")

inputs

from transformers import SpeechT5HifiGan

vocoder = SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")
speech = model.generate_speech(inputs["input_ids"], speaker_embeddings, vocoder=vocoder)

Audio(speech.numpy(),rate=16000)